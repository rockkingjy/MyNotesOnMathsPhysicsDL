%\documentclass[12pt,draft]{article}
\documentclass[12pt]{article}
\AtBeginDvi{\input{zhwinfonts}}
\usepackage{mathrsfs}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{listings}
\usepackage[dvips]{graphicx}
\usepackage{subfigure}
\usepackage[font=small]{caption}
\usepackage{threeparttable}
\usepackage{cases}
\usepackage{multicol}
\usepackage{url}
\usepackage{amsmath}
\usepackage{commath}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{overpic}
\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
%\usepackage[round]{natbib}
\usepackage{graphicx}
\numberwithin{equation}{section}
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}
\setlength{\parskip}{0.3\baselineskip}
\setlength{\headheight}{15pt}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}
\setlength{\parindent}{4ex}
\begin{document}\small
\title{Recent Visual Tracking Methods}
\author{Yan JIN}
\pagestyle{fancy}\fancyhf{}
\lhead{}\rhead{JIN Yan}
\lfoot{\textit{}}\cfoot{}\rfoot{\thepage}
\renewcommand{\headrulewidth}{1.pt}
\renewcommand{\footrulewidth}{1.pt}
\maketitle
\tableofcontents
%=======================================
\section{HOG feature \cite{dalal2005histograms} \cite{felzenszwalb2010object}} 
\subsection{Original HOG feature  \cite{dalal2005histograms} } \label{ch:hog}
Basic idea: local object appearance and shape can often be characterized rather well
by the distribution of local intensity gradient or edge directions, even without precise knowledge
of the corresponding gradient or edge position. \par
Input - detection window with size: $64 \times 128$. 
\begin{enumerate}
	\item Divide the detection window into $16 \times 16$ blocks of $50\%$ overlap.
		\begin{itemize}
			\item $7 \times 15 = 105$ blocks in total ($(64 \div (16 \div 2) - 1) \times 
				(128 \div (16 \div 2) - 1)$. 
		\end{itemize}
	\item Each block consists of $2 \times 2$ cells with size 8 pixels $ \times $ 8 pixels.
	\item Compute gradients for each pixel.
		\begin{itemize}
			\item Using $[-1, 0, 1]$ gradient filter.
		\end{itemize}
	\item Quantize the gradient orientation into 9 orientations bins in $0^\circ-180^\circ$
	(with $20^\circ$ interval).
		\begin{itemize}
			\item The vote is the gradient magnitude. \\
			For color images, calculate separate gradients for each color channel, and take the one with 
			the largest norm at the pixel's gradient vector.
			\item Interpolate votes bi-linearly between neighboring bin center. \\
			 For example, suppose a pixel have orientation $80^\circ$, next to it has $70^\circ$ and 
			 $90^\circ$ bins, then: $85-70=15$, $90-85=5$, so, for bin $70^\circ$ it contributes:
			  $15/(15+5)=3/4 \times$ gradient magnitude, and for bin $90^\circ$: $5/(15+5)=1/4 \times$
			  gradient magnitude.
			\item The vote can also be weighted by Gaussian to down-weight near the edge.
		\end{itemize}
	\item For each cell accumulating the 1-D histogram of gradient directions or edge orientations over
	the pixels of the cell.
	\item Normalization for blocks, for better invariance to illumination, shadowing etc..
		\begin{itemize}
			\item L2-Hys (Lowe-style clipped L2 norm) block normalization. That is, L2-norm followed by 
			clipping (limiting the maximum values of v to 0.2) and renormalizing.
		\end{itemize}		
	\item Concatenate histograms.
		\begin{itemize}
			\item Feature dimension: $36(=4 \times 9)$ for each block (in this example, 105 blocks in total,
			so $36 \times 105$ features in total for this input detection window).
		\end{itemize}	
\end{enumerate}
%--------------------------------
\subsection{Improved HOG feature \cite{felzenszwalb2010object}}
Basic idea: using 13-dimensional feature instead of previous 36-dimensional with no significant effect, and
add  contrast sensitive and contrast insensitive features to a 31-dimensional feature vector to improve the 
performance. \par
Input - image size: $w \times h$.
\begin{enumerate}
	\item Pixel-Level Feature Maps. 
		\begin{itemize}
			\item For each pixel (x, y), let $\theta(x,y)$ be the orientation and $r(x,y)$ be the magnitude of 
			the intensity gradient using $[-1, 0, 1]$ as gradient filter.
			\item The gradient calculated at the pixel is discretized into one of p values by contrast sensitive
			($B_1$) or contrast insensitive ($B_2$):
				\begin{equation}
					B_1(x,y) = round(\frac{p \theta(x,y)}{2 \pi})\mod p 
				\end{equation}
				\begin{equation}
					B_2(x,y) = round(\frac{p \theta(x,y)}{\pi})\mod p
				\end{equation}
			\item The feature vector $F(x, y)_b$ at pixel (x, y) is: 
				\begin{equation}
					F(x,y)_b=
					\begin{cases}
						r(x,y) & \text{if } b = B(x,y) \\
						0 & \text{otherwise}
    					\end{cases}
				\end{equation}
			where $b \in \{0, \cdots , p-1 \}$.
			\item Now we have a pixel-level feature map for the image.
		\end{itemize}
	\item Spatial Aggregation.
		\begin{itemize}
			\item Let $k > 0$ be the cell size.
			\item Aggregate pixel-level feature vectors to obtain a cell-based feature map C(i,j).
			\item In C(i,j), where $0 \le i \le \lfloor{(w-1)/k}\rfloor$, $0 \le j \le \lfloor{(h-1)/k}\rfloor$.
			\item Pixel (x, y) maps to $(\lfloor{x/k}\rfloor, \lfloor{y/k}\rfloor)$.
			\item The feature vector at a cell is the sum (or average) of the pixel-level features in that cell.
			\item Each pixel contributes to the feature vectors in the four cells around it using 
			bilinear interpolation.
			\item This aggregation provides some invariance to small deformations and reduces the size
			of a feature map.
		\end{itemize}
	\item Normalization and Truncation.
		\begin{itemize}
			\item Normalization provides the invariance to gain, while gradients provides invariant to bias.
			\item Let $T_{\alpha}(v)$ denote the truncation of a vector  v by $\alpha$.
			\item The feature map is obtained by concatenating the result of normalizing the map C with respect
			 to each normalization factor followed by truncation:
			 	\begin{equation}
					H(i,j)=
					\begin{bmatrix}
						T_{\alpha}(C(i,j)/N_{-1, -1}(i,j)) \\
						T_{\alpha}(C(i,j)/N_{+1, -1}(i,j)) \\
						T_{\alpha}(C(i,j)/N_{+1, +1}(i,j)) \\
						T_{\alpha}(C(i,j)/N_{-1, +1}(i,j)) \\
					\end{bmatrix}
				\end{equation}
			\item We use four normalization factors for C(i,j): $N_{\delta, \gamma}(i,j)$ with 
			$\delta, \gamma \in \{-1, 1\}$,
				\begin{equation}
					N_{\delta, \gamma}(i,j)=(\norm{C(i,j)}^2+\norm{C(i+\delta,j)}^2+
									\norm{C(i,j+\gamma)}^2+\norm{C(i+\delta,j+\gamma)}^2)^{1/2}
				\end{equation}
			Each factor measures the "gradient energy" in a square block of four cells containing (i,j).
			\item Commonly, HOG features using $p=9$, discretized with $B_2$, cell size $k=8$ and truncation 
			$\alpha = 0.2$, so we have 36-dimensional feature vector (9 orientations $ \times 4$ 
			normalizaions).
		\end{itemize}		
	\item PCA and Analytic Dimensionality Reduction.
		\begin{itemize}
			\item 9 contrast insensitive orientations, 18 contrast sensitive orientations, 4 normalization factors, 
			so we have $4 \times (9+18)=108$ dimensional feature vectors. 
			\item Use an analytic projection of 108 dimensional features to:
				\begin{enumerate}
			 		\item 27 (=9+18) sums over 4 different normalizations.
					\item 4 dimensional features sums over 9 contrast insensitive orientations. 
				\end{enumerate}
			\item In total, the final feature map has 31-dimensional vectors (27+4).
		\end{itemize}	
\end{enumerate}
%=======================================
\section{CN feature \cite{van2007learning}}
Color names (CN) are linguistic color labels assigned by humans to represent colors in the world. In English, it contains eleven basic color terms: black, blue, brown, green, orange, pink, purple, red, white and yellow. \par
Color naming is an operation that associates RGB observations with linguistic color labels.  \par
In \cite{van2007learning}, it provides the mapping automatically learned from images retrieved with Google-image search. It maps the RGB values to a probabilistic 11 dimensional color representation which sums up to 1. \par
%=======================================
\section{Deep feature \cite{chatfield2014return}}
	\begin{table}[h!]
  		\centering
  		\begin{tabular}{c|c|c|c|c|c|c|c}
   			Layer & Name & size & stride&rFieldStride& pad        & input         & output\\
   			\hline
			0        & Input   &[224,224,3,10]&           &1&              & 224 x 224 & 224 x 224\\ 
			1        & conv1 & [7,7,3,96]       &[2, 2]   &1& [0,0,0,0] & 224 x 224 & 109 x 109\\ 
			2        & relu1   &                       &            &2&               & 109 x 109 & 109 x 109\\ 
	     \textbf{3}  & \textbf{norm1} &                       &           &2&                & 109 x 109 & 109 x 109\\ 
			4        & pool1  & [3, 3]              & [2, 2]   &2& [0,1,0,1]  & 109 x 109  & 54 x 54\\ 
			5        & conv2 & [5,5,96,256]   & [2, 2]   &4&[1,1,1,1]   &  54 x 54    & 26 x 26 \\ 
			6        & relu2   &                       &            &8&                &  26 x 26 &  26 x 26\\ 
			7        & norm2 &                       &            &8&                &  26 x 26 &  26 x 26\\ 
			8        & pool2  & [3, 3]              & [2, 2]   &8& [0,1,0,1]  &  26 x 26 & 13 x 13\\ 
			9        & conv3 & [3,3,256,512] & [1, 1]   &16&[1,1,1,1]   & 13 x 13 & 13 x 13\\ 
			10      & relu3   &                       &            &16&                & 13 x 13 & 13 x 13\\
			11      & conv4 & [3,3,512,512]  & [1, 1]  &16&[1,1,1,1]   & 13 x 13 & 13 x 13\\ 
			12      & relu4   &             		&          &16&                 & 13 x 13 & 13 x 13\\ 
			13      & conv5 & [3,3,512,512]  & [1, 1] &16&[1,1,1,1]    & 13 x 13 & 13 x 13\\ 
	     \textbf{14} & \textbf{relu5}   &                        &          &16&                 & 13 x 13 & 13 x 13\\  
  		\end{tabular}
	\end{table} \par
Equation to calculate output size from input for convolution layer:
\begin{equation}
	\abs{Ouput} = \lfloor{\frac{\abs{Input} - size + pad1 + pad2}{stride}}\rfloor + 1
\end{equation}	

%=======================================
\section{MOSSE \cite{bolme2010visual}}
%=======================================
\section{CSK \cite{henriques2012exploiting}}

%=======================================
\section{KCF / DCF \cite{henriques2015high}}

%=======================================
\section{C-COT \cite{DanelljanECCV2016}}
\subsection{Preliminaries}
\subsubsection{Basic Notations}
Complex-value functions $g, h: \mathbb{R} \rightarrow \mathbb{C}$ are periodic with period $T > 0$. \par
Space $L^2(T)$ : Hilbert space equipped with an inner product $\langle \cdot , \cdot \rangle$ of periodic functions with period $T>0$. \par
For $g, h \in L^2(T)$, 
\begin{equation}\label{eq:conj}
	\langle g,h \rangle = \frac{1}{T} \int^{T}_{0} g(t) \overline{h(t)} dt
\end{equation}
where bar means complex conjugation. \par
Complex exponential functions: 
\begin{equation}
	e_k(t) = e^{i 2\pi kt/T}
\end{equation} \par
$\{e_k(t)\}^{\infty}_{-\infty}$ forms an orthonormal basis for $L^2(T)$. \par

%--------------------------------------------------
\subsubsection{Matrix Derivatives \cite{nasrabadi2007pattern}}
\begin{equation} \label{eq:matrixderivativ1}
	\frac{\partial}{\partial \bm{x}}(\bm{x^Ha}) = 
	\frac{\partial}{\partial \bm{x}}(\bm{a^Hx})= \bm{a}
\end{equation}

\begin{equation} \label{eq:matrixderivativ2}
	\frac{\partial}{\partial \bm{x}}(\bm{AB}) = 
	\frac{\partial \bm{A}}{\partial \bm{x}} \bm{B} + \bm{A} \frac{\partial \bm{B}}{\partial \bm{x}} 
\end{equation}

\begin{equation}\label{eq:matrixderivativ3}
	\frac{\partial}{\partial \bm{x}}(\bm{A^{-1}}) = 
	\bm{A}^{-1} \frac{\partial \bm{A}}{\partial \bm{x}} \bm{A}^{-1}
\end{equation}

\begin{equation}\label{eq:matrixderivativ4}
	\frac{\partial}{\partial \bm{x}}(\norm{\bm{Ax}}^2) 
	= \frac{\partial}{\partial \bm{x}} (\bm{x^HA^HAx} )
	= \bm{A^HAx + A^HAx} = 2\bm{A^HAx}
\end{equation}
%--------------------------------------------------
\subsubsection{Fourier transformation}
Fourier transformation:
\begin{equation}\label{eq:fourierTrans}
	\hat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\ e^{-2\pi i x \xi}\,dx
\end{equation} \par
Inverse Fourier transformation: 
\begin{equation}
	f(x) = \int_{-\infty}^{\infty} \hat f(\xi)\ e^{2 \pi i x \xi}\,d\xi
\end{equation} \par

For discrete points: $\left \{ \mathbf{ x_n } \right \} := x_0, x_1, \ldots, x_{N-1}$ and 
$\left \{ \mathbf{X_k} \right \} := X_0, X_1, \ldots, X_{N-1}$, we have: \par
Discrete Fourier transformation: 
\begin{equation}\label{eq:fourierdisc}
	X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i2\pi kn/N}
\end{equation} \par
Inverse discrete Fourier transformation
\begin{equation}
x_n = \frac{1}{N}\sum_{k=0}^{N-1}X_k \cdot e^{i 2 \pi kn/N} 
\end{equation} \par 

Fourier transformation of $g \in L^2(T)$: 
\begin{equation}\label{eq:fourierL2T}
	\hat{g}[k] = \langle g , e_k \rangle = \frac{1}{T} \int^{T}_{0} g(t)e^{-i 2\pi kt/T} dt
\end{equation} \par

Any $g \in L^2(T)$ can be expressed as: $g(t)=\sum^{\infty}_{-\infty}\hat{g}[k] e_k$. \par
\textbf{Shift property:} For any real number $L$, if $h(t)=g(t-L)$, then
\begin{equation} \label{eq:fouriershift}
	\hat{h}[k]=e^{-2\pi i L k} \hat{g}[k]
\end{equation}

\textbf{Parseval's formula:}
\begin{equation} \label{eq:parseval}
	\norm{g}^2_{L^2}=\norm{\hat{g}}^2_{l^2}
\end{equation}
where $\norm{g}^2= \langle g , g \rangle$ and $\norm{\hat{g}}^2_{l^2}=\sum^{\infty}_{-\infty}\abs{\hat{g}[k]}^2$. \par

\textbf{Poisson summation formula:}
\begin{equation}
	\sum_{n=-\infty}^\infty f(n)=\sum_{k=-\infty}^\infty \hat f\left(k\right)
\end{equation} \par
\begin{equation} \label{eq:poisson}
	\sum_{n=-\infty}^{\infty} s(t + nT)=\sum_{k=-\infty}^{\infty} \frac{1}{T}\cdot \hat s\left(\frac{k}{T}\right)e^{i 2\pi \frac{k}{T} t }
\end{equation} \par
%--------------------------------------------------
\subsubsection{FFT}

%--------------------------------------------------
\subsubsection{Convolution}
Convolution:
\begin{align} \label{eq:conv}
	(f * g )(t) &\stackrel{\mathrm{def}}{=}\ \int_{-\infty}^\infty f(\tau) g(t - \tau) \, d\tau \\
	&= \int_{-\infty}^\infty f(t-\tau) g(\tau)\, d\tau
\end{align} \par

Discrete convolution: 
\begin{align}
	(f * g)[k] &= \sum_{l=-\infty}^\infty f[l] g[k - l] \\
			&= \sum_{l=-\infty}^\infty f[k-l] g[l]
\end{align} \par

Circular convolution operation(with normalization) $*:L^2(T) \times L^2(T) \rightarrow L^2(T)$ here is defined by:
\begin{equation} \label{eq:circonv}
	 (g * h)(t) = \frac{1}{T} \int^{T}_{0} g(t-s) h(s) ds
\end{equation} \par
Convolution properties: 
\begin{equation}\label{eq:convprop}
	 \widehat{g*h}=\hat{g}\hat{h},\ \ \ \ \  \widehat{gh}=\hat{g}*\hat{h}
\end{equation} \par
To calculate discrete convolution, one of the inputs is converted into a \textbf{Toeplitz matrix}:
\begin{equation} \label{eq:convToep}
 y = h \ast x =
            \begin{bmatrix}
                h_1 & 0 & \ldots & 0 & 0 \\
                h_2 & h_1 & \ldots & \vdots & \vdots \\
                h_3 & h_2 & \ldots & 0 & 0 \\
                \vdots & h_3 & \ldots & h_1 & 0 \\
                h_{m-1} & \vdots & \ldots & h_2 & h_1 \\
                h_m & h_{m-1} & \vdots & \vdots & h_2 \\
                0 & h_m & \ldots & h_{m-2} & \vdots \\
                0 & 0 & \ldots & h_{m-1} & h_{m-2} \\
                \vdots & \vdots & \vdots & h_m & h_{m-1} \\
                0 & 0 & 0 & \ldots & h_m
            \end{bmatrix}_{(m+n-1) \times n}
            \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3 \\
                \vdots \\
                x_n
            \end{bmatrix}
\end{equation}
or 
\begin{equation}
            \begin{bmatrix}
                h_1 & h_2 & h_3 & \ldots & h_{m-1} & h_m
            \end{bmatrix}
            \begin{bmatrix}
                x_1 & x_2 & x_3 & \ldots & x_n & 0 & 0 & 0& \ldots & 0 \\
                0 & x_1 & x_2 & x_3 & \ldots & x_n & 0 & 0 & \ldots & 0 \\
                0 & 0 & x_1 & x_2 & x_3 & \ldots & x_n & 0  & \ldots & 0 \\
                \vdots & \vdots & \vdots & \vdots & \vdots & \ldots & \vdots & \vdots  & \ldots & 0 \\
                0 & \ldots & 0 & 0 & x_1 & \ldots & x_{n-2} & x_{n-1} & x_n & \vdots \\
                0 & \ldots & 0 & 0 & 0 & x_1 & \ldots & x_{n-2} & x_{n-1} & x_n
            \end{bmatrix}_{m \times (m+n-1)}
\end{equation}


%--------------------------------------------------
\subsubsection{Newton method} \label{ch:newtonmethod}

%--------------------------------------------------
\subsubsection{Conjugate Gradient \cite{shewchuk1994introduction}} \label{ch:conjugategradient}

%--------------------------------------------------
\subsection{Continuous Learning Formulation}
$x_j$: Training Samples. \par
$d \in \{1, \cdots , D\}$: Feature channel of training samples (ex. if choose deep layer 1, 5 as features, then $D = 2$). \par
$N_d$: Number of spatial sample points in feature channel d (ex. for HOG feature $N_{HOG}=31 \times 105$ \autoref{ch:hog}). \par
$\chi=\mathbb{R}^{N_1} \times \cdots \times \mathbb{R}^{N_D}$. \par
$\{x^d_j[n]\}, n \in \{0, \cdots, N_d -1\}$: Feature points in channel d of training sample $x_j$.\par
$[0, T) \subset \mathbb{R} $: Spatial support of the feature map, where $T$ is the size of the support region.\par
$J_d$: Interpolation operator of training sample x in feature channel d, defined as: 
\begin{equation}
	J_d\{x^d\}(t)=\sum^{N_d-1}_{n=0} x^d[n] b_d(t-\frac{T}{N_d} n)
\end{equation} \par
$f^d \in L^2(T)$ is the continuous filter for feature channel d. \par
$S_f: \chi \rightarrow L^2(T)$: maps a sample $x \in \chi$ to a confidence score function defined on the interval $[0, T)$, defined as: 
\begin{equation} \label{eq:score}
	S_f\{x\}=\sum^D_{d=1} f^d * J_d\{x^d\}, \  x \in \chi.
\end{equation} 
the convolution here is the circular convolution in continuous domain as defined in (\ref{eq:conv}). \par
The target is localized by maximizing the confidence score in an image region. \par

The filter $f$ is trained by minimizing the functional: 
\begin{equation} \label{eq:target}
	E(f)=\sum^{m}_{j=1} \alpha_j \norm{S_f\{x_j\}-y_j}^2 + \sum^D_{d=1} \norm{w f^d}^2
\end{equation}

%--------------------------------------------------
\subsection{Training the filter $f$ in Fourier domain}
To train the filter f, we minimize the function (\ref{eq:target}) in the Fourier domain. \par
By (\ref{eq:fouriershift}) and (\ref{eq:fourierdisc}), we have:
\begin{equation}
	\widehat{J_d\{x^d\}}[k]=\sum^{N_d-1}_{n=0} x^d[n] e^{-i\frac{2\pi}{N_d}kn} \hat{b}_d[k]=X^d[k] \hat{b}_d[k] 
\end{equation} \par
With the property of (\ref{eq:convprop}), we have:
\begin{equation} \label{eq:scoreFourier}
	\widehat{S_f\{x\}}[k]= \sum^D_{d=1} \hat{f}^d[k] X^d[k] \hat{b}_d[k] 
\end{equation} \par
By using Parseval's formula (\ref{eq:parseval}):
\begin{equation} \label{eq:targetFourier}
	E(f)=\sum^{m}_{j=1} \alpha_j \norm{\sum^D_{d=1}\hat{f}^dX^b_j\hat{b}_d-\hat{y}_j}^2_{l^2}
		 + \sum^D_{d=1} \norm{\hat{w} * \hat{f}^d}^2_{l^2}
\end{equation}
The functional $E(f)$ can be minimized with respect of $\hat{f}^d[k]$. \par 

%--------------------------------------------------
\subsection{Training the filter $\hat{\bm{f}}$ as finite matrix}
In practice the filter $f$ needs to be represented by a \textbf{finite} set. So we obtain a \textbf{finite} representation by minimizing (\ref{eq:targetFourier}) over the finite-dimensional subspace $V = span\{e_k\}^{K_1}_{-K_1} \times \cdots \times span\{e_k\}^{K_D}_{-K_D} \subset L^2(T)^D$, by assuming $\hat{f}^d[k] = 0$ for $\abs{k} > K_d$. Here we set $K_d = \lfloor \frac{N_d}{2} \rfloor$. \par
Define: 
\begin{equation}
	\hat{\bm{f}}^d= 
	\begin{bmatrix}
		\hat{f}^d[-K_d] \\  \vdots \\ \hat{f}^d[K_d] 
	\end{bmatrix}_{(2K_d+1) \times 1}
	 \in \mathbb{C}^{2K_d+1} \text{,   }
	\hat{\bm{f}}= 
	\begin{bmatrix}
		\hat{\bm{f}}^1 \\ \hat{\bm{f}}^2 \\ \vdots \\ \hat{\bm{f}}^D
	\end{bmatrix}_{\sum^{D}_{d=1}(2K_d+1) \times 1}
	 \text{,   }
	 \hat{\bm{y}}_j= 
	\begin{bmatrix}
		\hat{y}_j[-K] \\  \vdots \\ \hat{y}_j[K] 
	\end{bmatrix}_{(2K+1) \times 1}
\end{equation}
where $K:=\max_dK_d$.  \par
And define: 
\begin{equation}
	A^d_j = 
	\begin{bmatrix}
		X^d_j[-K_d]\hat{b}_d[-K_d] & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & X^d_j[K_d]\hat{b}_d[K_d] \\
	\end{bmatrix}_{(2K_d+1)\times(2K_d+1)}
\end{equation} \par
\begin{equation}
	A_j = 
	\begin{bmatrix}
		0_{(K-K_1)\times(2K_1+1)} & \cdots & 0_{(K-K_D)\times(2K_D+1)}  \\
		A_j^1 & \cdots & A_j^D \\
		0_{(K-K_1)\times(2K_1+1)} & \cdots & 0_{(K-K_D)\times(2K_D+1)} 
	\end{bmatrix}_{(2K+1)\times \sum^D_{d=1}(2K_d+1)}
\end{equation} \par
Let $L$ be the number of non-zero coefficients $\hat{w}[k]$, such that  $\hat{w}[k]=0$ for all $\abs{k}>L$. \par
Define $W_d$ to be the $(2K_d+2L+1) \times (2K_d+1)$ Toeplitz matrix corresponding to 
the convolution operator $W_d\hat{\bm{f}}^d=vec \ \hat{w}*\hat{f}^d$ (\ref{eq:convToep}):
\begin{equation} 
	W_d\hat{\bm{f}}^d =
	\begin{bmatrix}
                \hat{w}[-K_d]     & 0 				& \ldots & 0 			& 0 \\
                \hat{w}[-K_d+1] & \hat{w}[-K_d] 		& \ldots & \vdots 		& \vdots \\
                \hat{w}[-K_d+2] & \hat{w}[-K_d+1] 	& \ldots & 0 			& 0 \\
                \vdots 		&  \hat{w}[-K_d+2] 	& \ldots &  \hat{w}[-K_d] 	& 0 \\
                 \hat{w}[K_d-1]  & \vdots 			& \ldots &  \hat{w}[-K_d+1] &  \hat{w}[-K_d] \\
                 \hat{w}[K_d] 	&  \hat{w}[K_d-1] 	& \vdots & \vdots 		&  \hat{w}[-K_d+1] \\
                0 			&  \hat{w}[K_d]		& \ldots &  \hat{w}[K_d-2] 	& \vdots \\
                0 			& 0 				& \ldots &  \hat{w}[K_d-1]	&  \hat{w}[K_d-2] \\
                \vdots 		& \vdots 			& \vdots &  \hat{w}[K_d] 	&  \hat{w}[K_d-1]\\
                0 			& 0 				& 0 	     & \ldots 		& \hat{w}[K_d]
	\end{bmatrix}_{(2K_d+2L+1) \times (2K_d+1)}
	\begin{bmatrix}
		\hat{f}^d[-K_d] \\  \vdots \\ \hat{f}^d[K_d] 
	\end{bmatrix}_{(2K_d+1) \times 1}
\end{equation} \par
Define $W=W_1 \oplus \cdots \oplus W_D$. \par

Then the matrix version of (\ref{eq:targetFourier}) becomes:
\begin{equation} \label{eq:targetMatrix}
	E_V(f)=\sum^{m}_{j=1} \alpha_j \norm{A_j \hat{\mathbf{f}}-\hat{\mathbf{y}}_j}^2_{2}
		 + \norm{W  \hat{\mathbf{f}}}^2_{2}
\end{equation} \par

%--------------------------------------------------
\subsection{The normal equations of $\hat{\bm{f}}$}
Define: 
\begin{equation}
	A = 
	\begin{bmatrix}
		A_1 \\ A_2 \\ \vdots \\ A_m
	\end{bmatrix}
	 \text{,   }
	\hat{\bm{y}}= 
	\begin{bmatrix}
		\hat{\bm{y}}_1 \\ \hat{\bm{y}}_2 \\ \vdots \\ \hat{\bm{y}}_m
	\end{bmatrix}
	 \text{,   }
	 \Gamma=\alpha_1 I  \oplus \cdots \oplus \alpha_m I
\end{equation} \par


The equation (\ref{eq:targetMatrix}) becomes:
\begin{align} \begin{split} \label{eq:targetDerivative}
	E_V(f)&=
		(A \hat{\mathbf{f}}-\hat{\mathbf{y}})^H \Gamma (A \hat{\mathbf{f}}-\hat{\mathbf{y}})
		 + \hat{f}^H W^HW \hat{f} \\
		 &= (\hat{\mathbf{f}}^H A^H - \hat{\mathbf{y}}^H)\Gamma (A \hat{\mathbf{f}}-\hat{\mathbf{y}})
		 + \hat{f}^H W^HW \hat{f} \\
		 &= \hat{\mathbf{f}}^H A^H \Gamma A \hat{\mathbf{f}} - \hat{\mathbf{f}}^H A^H \Gamma \hat{\mathbf{y}}
		 - \hat{\mathbf{y}}^H \Gamma A \hat{\mathbf{f}} + \hat{\mathbf{y}}^H \Gamma \hat{\mathbf{y}}
		 + \hat{f}^H W^HW \hat{f} 
\end{split}\end{align} \par
Do the derivative to $\hat{\bm {f}}$ for the equation(\ref{eq:targetDerivative}) by using the properties (\ref{eq:matrixderivativ1}) and (\ref{eq:matrixderivativ4}):
\begin{align}
	A^H \Gamma (A \hat{\bm{f}} - \hat{\bm{y}})  
	+ W^HW\hat{\bm{f}} = 0
\end{align}

So the minimizer of (\ref{eq:targetMatrix}) is found by solving the equations:
\begin{equation} \label{eq:normal}
	(A^H \Gamma A + W^H W) \hat{\bm{f}} = A^H \Gamma \hat{\bm{y}}
\end{equation}
%--------------------------------------------------
\subsection{The desired output $y_j$}
$g_T(t)$: T-periodic repetition of function g, defined as:
\begin{equation} \label{eq:periodic}
	g_T(t) = \sum^{\infty}_{n=-\infty} g(t-nT)
\end{equation} \par
$\hat{g}_T[k]$: Fourier transformation of $g_T$, defined as:
\begin{align}\begin{split}\label{eq:fouriergT}
	\hat{g}_T[k] 
	&\stackrel{(\ref{eq:fourierL2T})}{=}  \langle g_T , e_k \rangle \\
	&\stackrel{(\ref{eq:conj})}{=} \frac{1}{T} \int^{T}_{0} g_T(t) e^{-i 2 \pi k t / T} dt \\ 
	&\stackrel{(\ref{eq:periodic})}{=} \frac{1}{T} \int^{T}_{0} \sum^{\infty}_{n=-\infty} g(t-nT) e^{-i 2 \pi k t / T} dt \\
	&\stackrel{(\ref{eq:poisson})}{=} \frac{1}{T} \int^{T}_{0} \frac{1}{T} \sum^{\infty}_{k'=-\infty} \hat{g}(\frac{k'}{T}) e^{i 2 \pi k' t / T}  e^{-i 2 \pi k t / T} dt \\
	&\stackrel{}{=} \frac{1}{T}  \int^{T}_{0} \sum_{k' \neq k} \hat{g}(\frac{k'}{T}) e^{i 2 \pi (k'-k) t / T} d(\frac{t}{T}) +  \frac{1}{T} \hat{g}(\frac{k}{T})\\
	&= Const \times e^{i 2 \pi (k'-k) t / T} |^T_0+  \frac{1}{T} \hat{g}(\frac{k}{T})\\
	&= \frac{1}{T} \hat{g}(\frac{k}{T})
\end{split}\end{align} \par
$y_j(t)=\sum^{\infty}_{n=-\infty}z_j(t-nT)$ is the periodic repetition of the Gaussian function $z_j(t)=e^{-\frac{1}{2\sigma^2}(t-u_j)^2}$, here $u_j \in [0, T)$ is the estimated location of the target in the corresponding sample $x_j$. \par
The Fourier transformation of $z_j(t)$ is: 
\begin{align}\begin{split}\label{eq:fourierz}
	\hat{z}_j[k]
	&\stackrel{(\ref{eq:fourierTrans})}{=} \int^{\infty}_{-\infty} z_j(t) e^{-i 2 \pi k t} dt \\ 
	&\stackrel{}{=} \int^{\infty}_{-\infty} e^{-\frac{1}{2\sigma^2}(t-u_j)^2} e^{-i 2 \pi k t} dt \\ 
	&\stackrel{}{=} \int^{\infty}_{-\infty} e^{-\frac{1}{2\sigma^2}\tau^2} e^{-i 2 \pi k (\tau+u_j)} d\tau \\ 
	&\stackrel{}{=} e^{-i 2 \pi k u_j} \int^{\infty}_{-\infty} e^{-\frac{1}{2\sigma^2}\tau^2} e^{-i 2 \pi k \tau} d\tau \\ 
	&\stackrel{}{=} e^{-i 2 \pi k u_j} e^{-2(\sigma \pi k)^2} \int^{\infty}_{-\infty} e^{-\frac{1}{2\sigma^2}(\tau+ i 2 \pi \sigma^2 k)^2} d\tau \\ 
	&\stackrel{}{=} \sqrt{2\pi\sigma^2}e^{-i 2 \pi k u_j} e^{-2(\sigma \pi k)^2} 
\end{split}\end{align} \par
So the Fourier transformation of $y_j(t)$ is:
\begin{align}\begin{split}
	\hat{y}_j[k]
	&\stackrel{(\ref{eq:fouriergT})}{=} \frac{1}{T} \hat{z}_j(\frac{k}{T}) \\
	&\stackrel{(\ref{eq:fourierz})}{=} \frac{\sqrt{2\pi\sigma^2}}{T}e^{-i 2 \pi \frac{k}{T} u_j-2(\sigma \pi \frac{k}{T})^2} 
\end{split}\end{align} \par
%--------------------------------------------------
\subsection{Interpolation function $b_d$}
Standard cubic spline kernel: 
\begin{equation}
	b(t)=
	\begin{cases}
		(a+2)\abs{t}^3-(a+3)t^2+1 & \abs{t} \leq 1 \\
		a\abs{t}^3-5at^2+8a\abs{t}-4a & 1 < \abs{t} \leq 2\\
		0	& \abs{t} > 2
    	\end{cases}
\end{equation} \par
First rescaling b to the sample interval $T/N_d$, then shifted half an interval $T/(2N_d)$ to align the origin of the continuous coordinate system with the sampling intervals of the feature map: 
\begin{equation}
	c_d(t)=b(\frac{N_d}{T}(t-\frac{T}{2N_d}))
\end{equation} \par
The Fourier transformation of it is:
\begin{align}\begin{split}\label{eq:fouriercd}
	\hat{c}_d(k) &= \int^{\infty}_{-\infty} c_d(t) e^{-i 2 \pi k t}dt \\
			&= \int^{\infty}_{-\infty} b(\frac{N_d}{T}(t-\frac{T}{2N_d})) e^{-i 2 \pi k t}dt \\
			&= \int^{\infty}_{-\infty} b(\frac{N_d}{T}\tau) e^{-i 2 \pi k (\tau + \frac{T}{2N_d} )} d\tau \\
			&= e^{-i\pi \frac{T}{N_d} k} \frac{T}{N_d} \int^{\infty}_{-\infty} b(\tau')e^{-i2\pi k \frac{T}{N_d} \tau' } d\tau' \\
			&= \frac{T}{N_d} e^{-i\pi \frac{T}{N_d} k} \hat{b}(\frac{T}{N_d}k) 
\end{split}\end{align} \par
$b_d(t)=\sum^{\infty}_{n=-\infty}c_d(t-nT)$  is the periodic repetition of $c_d(t)$, it's Fourier transformation is:
\begin{align}\begin{split}
	\hat{b}_d[k] 
	&\stackrel{(\ref{eq:fouriergT})}{=} \frac{1}{T} \hat{c}_d(\frac{k}{T}) \\
	&\stackrel{(\ref{eq:fouriercd})}{=}\frac{1}{N_d}e^{-i\frac{\pi}{N_d}k} \hat{b}(\frac{k}{N_d})
\end{split}\end{align} \par
where the Fourier transformation of $b(t)$ is:
\begin{equation}
	\hat{b}[k] = \frac{6(1-cos2\pi k)+3a(1-cos4\pi k) - (6+8a) \pi k \sin 2 \pi k - 2a \pi k \sin 4\pi k}{4k^4\pi^4}
\end{equation}

%-----------------------------------------------
\subsection{Spatially Regularization $W$ \cite{danelljan2015learning}}
To resolve the problem of unwanted boundary effects of discriminatively learned correlation filters, a spatially regularization is introduced:
\begin{equation}
	w(m, n) = \mu + \eta(m/P)^2 + \eta(n/Q)^2
\end{equation}
where $P \times Q$ denotes the target size. \par
%-----------------------------------------------
\subsection{Tracking Frameworks}
\subsubsection{Localization}
\begin{enumerate}
	\item Extract the feature maps $\{x^d_j[n]\}: n \in \{0, \cdots, N_d -1\}, d \in \{1, \cdots, D\}$ from the region of interest in an image.
	\item Calculate confidence score $s=S_f\{x\}$ using (\ref{eq:scoreFourier}) and inverse DFT. 
	\item To maximizing the score $s(t): t\in [0,T)$,
		\begin{enumerate}
			\item Using grid search on $s(T\frac{n}{2K+1})$ for $n=0, \cdots, 2K$ to find a rough
			 initial estimate $s(t)$.
			\item Do Fourier series expansion $s(t)=\sum^K_{-K}\hat{s}[k]e_k(t)$, and use the result above
			as the initial state, using Newtons method (\ref{ch:newtonmethod}) to do iterative optimization of it.
		\end{enumerate}
\end{enumerate}
\subsubsection{Training}
Using Conjugate Gradient (\ref{ch:conjugategradient}) to iteratively solving the equation (\ref{eq:normal}).
\subsubsection{Parameters}
Check file params.h in the code \url{https://github.com/rockkingjy/OpenTrackers}. 
%=======================================
\section{ECO \cite{DanelljanCVPR2017}} 
\subsection{Preliminaries}
\subsubsection{Kronecker product}
If \textbf{A} is an $m \times n$ matrix and \textbf{B} is a $p \times q$ matrix, then the Kronecker product $A \otimes B$ is the $mp  \times nq$ block matrix:
\begin{equation}
	\mathbf{A}\otimes\mathbf{B} = 
	\begin{bmatrix} 
		a_{11} \mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ 
		\vdots & \ddots & \vdots 
		\\ a_{m1} \mathbf{B} & \cdots & a_{mn} \mathbf{B} 
	\end{bmatrix}
\end{equation} \par
\subsubsection{Forbenius norm} \label{ch:forbenius}
\begin{equation}
	\|A\|_{\rm F} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2} = \sqrt{\operatorname{trace}(A^\dagger A)} = \sqrt{\sum_{i=1}^{\min\{m, n\}} \sigma_i^2(A)}
\end{equation}
\subsubsection{Gauss-Newton Method} \label{ch:gaussnewton}
%-----------------------------------------------
\subsection{Factorized Convolution Operator}
\subsubsection{Factorized convolution}
Define: \par
$\hat{f}_i$ and $P_i$ means the value of $i$th iteration. \par
$P=(p_{d,c})_{D \times C}$. \par
$\hat{z}^d[k]=X^d[k]\hat{b}_d[k]$. \par
Instead of learning one separate filter of each feature channel $d$, we use a smaller set of basis filters 
$f^1, \cdots, f^C$, where $C < D$. Then the filter for any feature layer $d$ could be constructed as: 
$f^d = \sum^C_{c=1} p_{d,c}f^c$. \par
Then the filter could be written as: $(Pf)_{D \times 1}$, so we have 
the factorized convolution operator from (\ref{eq:score}), with the linearity property of convolution, \par
\begin{equation}
	S_{Pf}\{x\}= Pf*J\{x\}=\sum_{c,d} p_{d,c} f^c * J_d\{x^d\} = f*P^TJ\{x\}
\end{equation} \par
Here $P^T$ resembles a linear dimensionality reduction operator. \par
We now learning the filter $f$ and $P^T$ jointly. \par
Add a extra regularization using the Forbenius norm (\$ \ref{ch:forbenius}) of P, and now using just a \textbf{single} sample, the loss of (\ref{eq:target}) becomes: 
\begin{equation} \label{eq:targetFactorized}
	E(f, P)=\norm{S_{Pf}\{x\}-\hat{y}}^2_{L^2}
		 + \sum^C_{c=1} \norm{w f^c}^2_{L^2}
		 + \lambda \norm{P}^2_F
\end{equation} \par
By applying the Parseval's formula (\ref{eq:parseval}), 
\begin{equation} \label{eq:targetFourierFactorized}
	E(f, P)=\norm{\hat{z}^TP\hat{f}-\hat{y}}^2_{l^2}
		 + \sum^C_{c=1} \norm{\hat{w} * \hat{f}^c}^2_{l^2}
		 + \lambda \norm{P}^2_F
\end{equation}
where $(\hat{z}^TP\hat{f})[k]=\sum^D_{d=1}\sum^C_{c=1}\hat{z}^d[k]p_{d,c}\hat{f}^c[k]$. \par
We use Gauss-Newton (\ref{ch:gaussnewton})  and Conjugate Gradient (\ref{ch:conjugategradient}) methods to optimize the quadratic subproblems. The Gauss-Newton method is derived by linearizing the residuals in {\ref{eq:targetFourierFactorized}) using a first order Taylor series expansion, 
\begin{align} \begin{split} \label{eq:approTayor}
	\hat{z}^T(P_i+\Delta P)(\hat{f}_i+\Delta\hat{f}) 
	&\approx \hat{z}^T P_i \hat{f}_{i, \Delta} +  \hat{z}^T \Delta P \hat{f}_{i} \\
	&= \hat{z}^T P_i \hat{f}_{i, \Delta} + (\hat{f}_i \otimes \hat{z})^T vec(\Delta P)
\end{split}\end{align} 
where $\hat{f}_{i, \Delta} = \hat{f}_i + \Delta \hat{f}$, and $\otimes$ is Kronecker product. \par
Substitute (\ref{eq:approTayor}) into (\ref{eq:targetFourierFactorized}):
\begin{equation} \label{eq:factorizedConvGaussNewton}
	\tilde{E}(\hat{f}_{i, \Delta}, \Delta P)
		=\norm{\hat{z}^TP_i\hat{f}_{i, \Delta}+ (\hat{f}_i \otimes \hat{z})^T vec(\Delta P)
 -\hat{y}_j}^2_{l^2}
		 + \sum^C_{c=1} \norm{\hat{w} * \hat{f}^c}^2_{l^2}
		 + \lambda \norm{P_i + \Delta P}^2_F
\end{equation} 
which is a linear least squares problem. \par
Then use Conjugate Gradient to optimize each Gauss-Newton subproblem to obtain the new filter 
$\hat{f}^*_{i, \Delta}$ and $\Delta P^*$, and the filter and matrix is updated as: $\hat{f}_{i+1}=\hat{f}^*_{i, \Delta}$
and $P_{i+1}=P_i+\Delta P^*$. \par
\subsubsection{Matrix version of Factorized Convolution Operator}
Define: 
\begin{equation}
	\hat{\bm{z}}^d= 
	\begin{bmatrix}
		\hat{z}^d[-K_d] \\  \vdots \\ \hat{z}^d[K_d] 
	\end{bmatrix}_{(2K_d+1) \times 1}
	\text{,   }
	\hat{\bm{z}}= 
	\begin{bmatrix}
		\hat{\bm{z}}^1 \\ \hat{\bm{z}}^2 \\ \vdots \\ \hat{\bm{z}}^D
	\end{bmatrix}_{\sum^{D}_{d=1}(2K_d+1) \times 1}
\end{equation} \par
\begin{equation}
	\hat{\bm{f}}_i^c= 
	\begin{bmatrix}
		\hat{f}^c_i[-K_c] \\  \vdots \\ \hat{f}^c_i[K_c] 
	\end{bmatrix}_{(2K_c+1) \times 1}
	\text{,   }
	\hat{\bm{f}}_i= 
	\begin{bmatrix}
		\hat{\bm{f}}^1_i \\ \hat{\bm{f}}^2_i \\ \vdots \\ \hat{\bm{f}}^C_i
	\end{bmatrix}_{\sum^{C}_{d=1}(2K_c+1) \times 1}
\end{equation} \par
\begin{equation}
	\hat{\bm{f}}_{i, \Delta}^c= 
	\begin{bmatrix}
		\hat{f}^c_{i, \Delta}[-K_c] \\  \vdots \\ \hat{f}^c_{i, \Delta}[K_c] 
	\end{bmatrix}_{(2K_c+1) \times 1}
	 \text{,   }
	\hat{\bm{f}}_{i, \Delta}= 
	\begin{bmatrix}
		\hat{\bm{f}}^1_{i, \Delta} \\ \hat{\bm{f}}^2_{i, \Delta} \\ \vdots \\ \hat{\bm{f}}^D_{i, \Delta}
	\end{bmatrix}_{\sum^{C}_{c=1}(2K_c+1) \times 1}	
	 \text{,   }
	 \hat{\bm{y}}_j= 
	\begin{bmatrix}
		\hat{y}_j[-K] \\  \vdots \\ \hat{y}_j[K] 
	\end{bmatrix}_{(2K+1) \times 1}
\end{equation}
\begin{equation}
	\bm{P}=
	\begin{bmatrix}
		p_{d, c}
	\end{bmatrix}_{\sum^{D}_{d=1}(2K_d+1) \times \sum^{C}_{c=1}(2K_c+1)}
	\text{, }
	\bm{P}\bm{\hat{f}_i}=
	\begin{bmatrix}
		p_{d, c}\bm{\hat{f}}^c_i 
	\end{bmatrix}_{\sum^{D}_{d=1}(2K_d+1) \times 1}
	\text{, }
	\bm{\hat{z}^TP\hat{f}_{i,\Delta}} \text{ is a scala.}
\end{equation}\par
\begin{equation}
	\bm{\hat{f}}_i \otimes \bm{\hat{z}}=
	\begin{bmatrix}
		\bm{\hat{f}^c_i\hat{z}^d}
	\end{bmatrix}_{\sum^{C}_{c=1}(2K_c+1)\sum^{D}_{d=1}(2K_d+1) \times 1}
	\text{, }
	 B_f= 
	\begin{bmatrix}
		(\bm{\hat{f}}_i \otimes \bm{\hat{z}})[-K]^T \\  \vdots \\ (\bm{\hat{f}}_i \otimes \bm{\hat{z}})[K]^T
	\end{bmatrix}_{(2K+1) \times \sum^{C}_{c=1}(2K_c+1)\sum^{D}_{d=1}(2K_d+1)}
\end{equation}
where $K:=\max_dK_c$.  \par
\begin{equation}
 	\bm{p}= 
	\begin{bmatrix}
		vec(p_{d,c})
	\end{bmatrix}_{\sum^{C}_{c=1}(2K_c+1)\sum^{D}_{d=1}(2K_d+1) \times 1}
	 \text{,   }
	 \bm{\Delta p}= 
	\begin{bmatrix}
		vec(\Delta p_{d,c})
	\end{bmatrix}_{\sum^{C}_{c=1}(2K_c+1)\sum^{D}_{d=1}(2K_d+1) \times 1}
\end{equation}
\begin{equation}
	A^c_j = 
	\begin{bmatrix}
		X^c_j[-K_c]\hat{b}_c[-K_c] \bm{p}_c & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & X^c_j[K_c]\hat{b}_c[K_c]  \bm{p}_c\\
	\end{bmatrix}_{(2K_c+1)\times(2K_c+1)}
\end{equation} \par
\begin{equation}
	A_P = 
	\begin{bmatrix}
		0_{(K-K_1)\times(2K_1+1)} & \cdots & 0_{(K-K_C)\times(2K_C+1)}  \\
		A_j^1 & \cdots & A_j^C \\
		0_{(K-K_1)\times(2K_1+1)} & \cdots & 0_{(K-K_C)\times(2K_C+1)} 
	\end{bmatrix}_{(2K+1)\times \sum^C_{c=1}(2K_c+1)}
\end{equation} \par
The the equation (\ref{eq:factorizedConvGaussNewton}) becomes:
\begin{equation} \label{eq:targetDerivativeECO}
	\tilde{E}(\hat{\bm{f}}_{i, \Delta}, \bm{\Delta p})
		=\norm{A_P\hat{\bm{f}}_{i, \Delta}+ B_f \bm{\Delta p} -\hat{\bm{y}}}^2_2
		 + \norm{W \hat{\bm{f}}_{i, \Delta}}^2_2
		 + \lambda \norm{\bm{p} + \bm{\Delta p}}^2_2
\end{equation} \par
Do the derivative to $\hat{\bm {f}}_{i, \Delta}$ and $\bm{\Delta p}$ for the equation(\ref{eq:targetDerivativeECO}) by using the properties (\ref{eq:matrixderivativ1}) and (\ref{eq:matrixderivativ4}):
\begin{equation}
	\begin{bmatrix}
		A^H_PA_P+W^HW & A^H_PB_f \\
		B^H_fA_P			& B^H_fB_f+\lambda I
	\end{bmatrix}
	\begin{bmatrix}
		\bm{\hat{f}_{i, \Delta}} \\
		\bm{\Delta p}
	\end{bmatrix}
	=
	\begin{bmatrix}
		A^H_P\bm{\hat{y}} \\
		B^H_f\bm{\hat{y}} -\lambda \bm{p}
	\end{bmatrix}
\end{equation} \par
Then we use Conjugate Gradient to iteratively solve this equation. 
%-----------------------------------------------
\subsection{Generative Sample Space Model}
\begin{equation} 
	E(f)=\mathbb{E} \{\norm{S_f\{x_j\}-y}^2_{L^2} \}+ \sum^D_{d=1} \norm{w f^d}^2_{L^2} 
\end{equation}

\begin{equation} 
	\pi_n = \pi_k + \pi_l , \mu_n = \frac{\pi_k\mu_k+\pi_l\mu_l}{\pi_k+\pi_l}
\end{equation}

\begin{equation} 
	E(f)=\sum^L_{l=1} \pi_l \norm{S_f\{\mu_l\}-y_0}^2_{L^2} + \sum^D_{d=1} \norm{w f^d}^2_{L^2} 
\end{equation}
%-----------------------------------------------
\subsection{Update strategy}
Update the filter by starting the optimization process in every $N_s$th frame instead of every time.
%=======================================
\section{Some tips of transfer from matlab to $c++$}
\begin{itemize}
	\item Debug tricks, to show the line number, file number and functions:
		\begin{lstlisting}[language=C++]
#define debug(a, args...) printf("%s(%s:%d) " a "\n", 
	__func__, __FILE__, __LINE__, ##args)
#define ddebug(a, args...) printf("%s(%s:%d) " a "\n", 
	__func__, __FILE__, __LINE__, ##args)
		\end{lstlisting}
	\item Debug tricks, to show the property of cv::mat:
		\begin{lstlisting}[language=C++]
void imgInfo(cv::Mat mat)
{
	int type = mat.type();
	string r;

	uchar depth = type & CV_MAT_DEPTH_MASK;
	uchar chans = 1 + (type >> CV_CN_SHIFT);

	switch (depth)
	{
	case CV_8U:
		r = "8U";
		break;
	case CV_8S:
		r = "8S";
		break;
	case CV_16U:
		r = "16U";
		break;
	case CV_16S:
		r = "16S";
		break;
	case CV_32S:
		r = "32S";
		break;
	case CV_32F:
		r = "32F";
		break;
	case CV_64F:
		r = "64F";
		break;
	default:
		r = "User";
		break;
	}

	r += "C";
	r += (chans + '0');

	debug("%s %d x %d", r.c_str(), mat.rows, mat.cols);
	//return r;
}
		\end{lstlisting}		
	\item Std functions better to name out to prevent default link to other functions. 
	For example: 
		\begin{lstlisting}[language=C++]
void absTest()
{
	std::vector<float> v{0.1, 0.2};
	float abs = abs(1.23f);
	debug("False result:%f", abs);

	abs = std::abs(1.23f);
	debug("True result:%f", abs);
}
		\end{lstlisting}
	For the False result, it sometimes link to other library and give answer 1.
	\item Take care of the parameter types, better to write it out clearly.
		\begin{lstlisting}[language=C++]
void accumulateTest()
{
	std::vector<float> v{0.1, 0.2};
	float sum = std::accumulate(v.begin(), v.end(), 0);
	debug("False result:%f", sum);

	sum = std::accumulate(v.begin(), v.end(), 0.0f);
	debug("True result:%f", sum);
}
		\end{lstlisting}
	For the False result, it gave answer 0, while the True result is 0.3.
	\item In opencv, color order: BGR, in matlab: RGB.
	\item In opencv mat, the data store in this order: $channel -> x -> y$. Test example:
		\begin{lstlisting}[language=C++]
void opencvTest()
{
	float *newdata = (float *)malloc(sizeof(float) * (2 * 3 * 4));
	for (int i = 0; i < 2 * 3 * 4; i++)
	{
		newdata[i] = i;
	}

	cv::Mat mat = cv::Mat(cv::Size(3, 4), CV_32FC(2), newdata);

	printf("\nInfo of original mat:");
	imgInfo(mat);
	for (int i = 0; i < 2 * 3 * 4; i++)
	{
		printf("%f ", mat.at<float>(0, i));
	}
	printf("\n");

	std::vector<cv::Mat> splitmat;
	cv::split(mat, splitmat);

	printf("\nInfo of splited mat:");
	imgInfo(splitmat[0]);

	printf("channel 0:\n");
	for (int j = 0; j < mat.rows; j++)
	{
		for (int i = 0; i < mat.cols; i++)
		{
			printf("%f ", splitmat[0].at<float>(j, i));
		}
		printf("\n");
	}
	printf("\n");
	printf("channel 1:\n");
	for (int j = 0; j < mat.rows; j++)
	{
		for (int i = 0; i < mat.cols; i++)
		{
			printf("%f ", splitmat[1].at<float>(j, i));
		}
		printf("\n");
	}
}
		\end{lstlisting}
	\item When using libcaffe.so, using the BLVC caffe version. I used the Nvidia version and GPU 
	not working, wasted my days.
\end{itemize}
%=======================================
\renewcommand\refname{Reference}
\bibliographystyle{ieeetr} %unsrtnat
\bibliography{VisualTracking} 
\clearpage
\end{document}